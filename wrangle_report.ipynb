{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba510a1c",
   "metadata": {},
   "source": [
    "Author: Olatunji gabriel<br/>\n",
    "Date: 29th, May 2022<br/>\n",
    "\n",
    "Data wrangling is an important first step in having a good analysis and building a machine learning model with data. Data wrangling involves the following three step: Gathering, Access and Clean. The importance of this documentation is to summarize my data wrangling process of the Weratedog twitter dataset. I followed the first step of data wrangling by first gathering all the needed data. <br/>\n",
    "The first data I collected was the data already archived by Udacity, which was made available to me for downloads. That data was easier for me to collect because it was made available by Udacity for downloads.<br/> \n",
    "The second dataset I gathered was the pet images dataset. It was located on Udacity server and so I had to use Python requests library to get the data on the server through the URL made available to me by Udacity. With reference to the lessons on the data wrangling course and searching of the internet, I was able to use the Python requests module to get access to the server. I then created a local folder in my machine and write the content on the server to a newly created file and saved it as .tsv file. The creation of the folder and the file was done programmatically.<br/>\n",
    "The last data I gathered was from twitter API. That was the only way I could get each pet favorite count and retweet count. This process was tedious because I had to create twitter developer account and waited for my created account to be approved.  After which I had access to twitter API, I got each pet favorite count and retweet count through pet ids available on the first dataset I gathered and wrote the json data for each pet to a .txt file. I read from the .txt file using Python file manager, converted the json data to Python dictionary and created a pandas dataframe from these information.<br/>\n",
    "After I finished gathering the data, I checked for tidiness of the data and found some features which are meant to be values of dog stage name as separate features. But before I tidy uo the data, I checked for data quality and then ensure all three dataset was cleaned. I the  handled the untidy issue of the data, after which i merged all the cleaned and tidy dataset together since they have a field (tweet id) common to them. I then saved the merged dataset on a database as .db file and also on my local computer as .csv file. This new dataframe was then imported for analysis and visualization.<br/><br/>\n",
    "Thank you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3daf86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
