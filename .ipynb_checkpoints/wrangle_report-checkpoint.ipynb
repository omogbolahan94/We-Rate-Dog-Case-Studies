{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba510a1c",
   "metadata": {},
   "source": [
    "Author: Olatunji gabriel<br/>\n",
    "Date: 29th, May 2022<br/>\n",
    "\n",
    "Data wrangling is an important first step in having a good analysis and building a machine learning model with data. Data wrangling involves the following three step: Gathering, Access and Clean. The importance of this documentation is to summarize my data wrangling process of the Weratedog twitter dataset. I followed the first step of data wrangling by first gathering all the needed data. <br/>\n",
    "The first data I collected was the data already archived by Udacity, which was made available to me for downloads. That data was easier for me to collect because it was made available by Udacity.<br/> \n",
    "For the pet images dataset: the second dataset I gathered,  it was located on Udacity server and so I had to make a get request using the URL made available to me by Udacity. With reference to the previous lessons and searching of the internet, I was able to use Python request module to locate the server. I then created a local folder in my machine and write the content on the server on a newly created file and saved it as .tsv file. The creation of the folder and the file was done programmatically.<br/>\n",
    "The last data I gathered was from twitter API. That was the only way I could get each pet favorite count and retweet count. This process was tedious because I had to create twitter developer account and waited for my registration to be approved.  After which I had access twitter API, I got each pet favorite count and retweet count through pet ids available on the first dataset I gathered and wrote the json data for each pet to a .txt file. After which I read from the .txt file, converted the json data to python dictionary and created a pandas dataframe from these information.<br/>\n",
    "After I finished gathering the data, I checked for tidiness of the data and then ensure the data was tidy. After which I checked for data quality and then ensure all three dataset was cleaned. I then merged all the cleaned and tidy dataset and saved the merged dataset on a database as .db file and also on my local computer as .csv file. This new dataframe is then imported for analysis and visualization.<br/>\n",
    "Thank you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3daf86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
